王伟超
wangweichao@tedu.cn
邮件：中心-班级-代码(附件)

第四阶段课程介绍 - 13个工作日
1、爬虫(SPIDER) - 7个工作日左右
   1.1> 爬虫工程师
   1.2> 作为其他岗位的附加技能(数据分析、后端开发)
2、Hadoop(大数据) - 3个工作日左右
   数据分析岗位的附加技能
3、数据结构(DataStructure) - 3个工作日左右
   所有技术岗位的附加技能

第四阶段课程特点
1、爬虫
   综合、不稳定性较强
   类、函数、字符串、列表、元组...
   MySQL、MongoDB、Redis
   多进程、多线程
   re正则表达式
   HTML
   JavaScript
   爬虫自身的巨量知识点
   图像OCR
2、Hadoop
   抽象
3、数据结构
   抽象


1、政府、公安机关的网站不要爬
2、涉及到个人隐私的数据(用户信息、简历)


1、爬虫分类
   1.1> 通用网络爬虫(搜索引擎)
   1.2> 聚焦网络爬虫(自己写的爬虫程序)
2、robots协议
   搜索引擎和网站之间签署的爬虫协议
   网站会通过robots协议来告诉搜索引擎,哪些页面可以抓,哪些页面不能抓,通用网络爬虫必须得遵守robots协议（君子协议）
3、模块使用
   resp = requests.get(url='', headers={'User-Agent':''})
   html = requests.get(url='', headers={'User-Agent':''}).text
   html = requests.get(url='', headers={'User-Agent':''}).content.decode('utf-8')
4、响应对象属性
   resp.text （字符串）
   resp.content （字节串）
   resp.status_code （HTTP响应码）
   resp.url （返回实际数据的URL地址）
5、解析模块（re）
   r_list = re.findall(regex, html, re.S)
   正则错误:  []
   正则1个分组: ['', '', '', '', ...]
   正则n个分组: [(), (), ...]
6、爬虫程序流程
   6.1> 确认数据来源：右键-查看网页源代码-搜索关键字
   6.2> 响应内容中存在：观察URL地址的规律,拼接多页的URL地址
   6.3> 正则表达式的编写
   6.4> 正常编写代码
7、数据持久化的方式
   7.1> MySQL(pymysql)
        __init__(self): 连接数据库
	save_html(self): 将数据处理成列表|元组,execute()方法
	crawl(self): 关闭游标,断开数据库的连接
   7.2> MongoDB
        __init__(self): 连接数据库(3个对象)
	save_html(self): 将数据处理成字典,存入数据库
	常用命令：mongo、show dbs、use 库名、show collections、db.集合名.count()、db.集合名.find().pretty()、db.dropDatabase()
   7.3> csv
        __init__(self): 打开文件并初始化写入对象
	save_html(self): 将数据处理成列表|元组,存入csv文件
	crawl(self): 关闭文件
8、多级页面数据抓取
   8.1> 定义功能函数
        请求功能函数、解析功能函数、md5加密功能函数... ...
   8.2> 以一级页面解析函数为主线函数
9、增量爬虫实现
   9.1> 利用redis中的集合
        redis基于内存,存取速度相当之快
	redis中自带集合数据类型,成员不能重复,适合存储请求指纹
   9.2> 原理：利用sadd()的返回值
        返回值1: 为新更新数据,需要抓取
	返回值0: 之前已抓过,结束程序!
10、目前遇到的反爬及解决方案
   10.1> 反爬: 基于headers反爬虫(User-Agent、Cookie、Referer)
         解决: 发送请求时,携带headers参数(F12抓包)
   10.2> 反爬: 基于User-Agent反爬虫 - 频率
         解决: 创建User-Agent池,每次访问随机选择UA
	       利用fake_useragent模块
	       from fake_useragent import UserAgent
	       agent = UserAgent().random
   10.3> 反爬: 对响应内容做调整(响应内容和前端页面HTML结构不一致)
         解决: 一切以响应内容为准,调整正则或xpath表达式
   10.4> 反爬: 基于IP地址频率反爬
         解决: 建立代理IP池,每次访问随机选择一个代理IP
	       (免费代理IP池、收费代理IP池)
   10.5> 反爬: JS加密 - 有道翻译
         解决: 抓取加密的JS文件,用Python实现一遍(salt、sign)
   10.6> 反爬: JS逆向(JS加密代码异常复杂) - 百度翻译
         解决: 找到对应加密的JS代码,利用pyexecjs模块进行逆向处理
   10.7> 反爬: URL地址跳转 - 民政部
         解决: 提取跳转后的URL地址,再发请求提具体数据
   10.8> 反爬: Ajax动态加载
         解决: F12抓包,确定返回实际数据的网络数据包进行分析
11、解析模块汇总
   11.1> 正则re ：r_list = re.findall(regex, html, re.S)
   11.2> lxml + xpath
         铭记: 只要调用了xpath,则结果一定是列表！！！！！
	 结果情况1: [] 空列表
	 结果情况2: [<element div at xxx>,<element div at xx>]
	            xpath表达式末尾没有 /text()  /@属性名
		    //div、//ul/li、//dl[@id="xxx"]/dd/div[1]/a[2]
	 结果情况3: ['', '', '', ....]
		    xpath表达式末尾有 /text()   /@属性名
		    //div/text()  //dl/dd[2]/a[1]/@href
   11.3> lxml + xpath 最常用
         eobj = etree.HTML(html)
	 div_list = eobj.xpath('//ul/li')
	 for div in div_list:
	     item = {}
	     item['name'] = div.xpath('.//p/text()')[0].strip()
12、requests模块方法及参数总结
   12.1> requests.get()
         url = ''
	 proxies = {}
	 headers = {}
	 timeout = 3
   12.2> requests.post()
         url = ''
	 data = {} #form表单数据
	 proxies = {}
	 headers = {} 
	 timeout = 3
13、数据抓取最终梳理
  确认数据来源 - 右键查看网页源代码搜索关键字
  13.1> 响应内容中存在的情况
        观察URL地址规律,想办法拼接多页的URL地址
	写正则表达式 或者 xpath表达式
	完善代码
  13.2> 响应内容中不存在的情况
	F12抓包工具进行抓包,在页面中执行某些行为
	努力寻找返回实际数据的网络数据包(All、XHR)
	分析具体网络数据包(Headers)
	如果有加密数据,进一步破解处理
14、pyexecjs模块使用 - 用于JS逆向执行JS代码
    import execjs
    with open('xxx.js', 'r') as f:
        jscode = f.read()
    # 1.创建js编译对象
    jsobj = execjs.compile(jscode)
    # 2.调用eval()方法执行JS代码
    jsobj.eval('')

15、selenium爬虫优缺点
   15.1> 优点
         简单,不用过多分析页面结构以及数据来源,操作真实的浏览器
   15.2> 缺点
         效率低(提取数据效率非常低、等待页面元素加载需要时间)
16、浏览器对象方法
    driver = webdriver.Chrome()
    16.1> driver.get(url='')
    16.2> driver.quit()
    16.3> driver.close() ：关闭当前页
    16.4> driver.maximize_window() ：浏览器窗口最大化
    16.5> driver.page_source ：HTML结构源码(不是响应内容)
    16.6> driver.page_source.find('字符串') ：查找失败,返回-1
17、节点对象方法
    node = driver.find_element_by_xxx('')
    17.1> node.send_keys('关键字') ：向文本框发送文本
    17.2> node.click() ：单击
    17.3> node.get_attribute('属性名') ：获取节点属性值
    17.4> node.text ：获取当前节点及子节点和后代节点的文本内容
18、定位节点方法
    18.1> driver.find_element_by_xxx('') 
          结果: 1个节点对象
    18.2> driver.find_elements_by_xxx('')
          结果: 节点对象列表 []
    by_id、by_name、by_class_name、by_xpath、by_link_text、by_partical_link_text
19、selenium常用流程
    19.1> 使用场景一 : text属性有规律情况
          dd_list = driver.find_elements_by_xxx('')
	  for dd in dd_list:
	      print(dd.text) # 观察规律
    19.2> 使用场景二 : text属性没有明显规律情况
          dd_list = driver.find_elements_by_xxx('')
	  for dd in dd_list:
	      # dd.text没有明显规律怎么办？？？？
	      item = {}
	      item['name'] = dd.find_element_by_xxx('')
	      item['score'] = dd.find_element_by_xxx('')





selenium高级

1、设置无界面模式
   options = webdriver.ChromeOptions()
   options.add_argument('--headless')
   driver = webdriver.Chrome(options=options)
2、操作鼠标
   from selenium.webdriver import ActionChains
   ActionChains(driver).move_to_element(node).perform()
   鼠标方法
   move_to_element(node)
   move_to_element_with_offset(node, xoffset=n, yoffset=n)
   move_by_offset(xoffset=n, yoffset=n)
   release()
   click_and_hold()
3、切换句柄
   li = driver.window_handles
   driver.switch_to.window(li[1])


课间小练习：
1、网易云音乐设置成 无界面模式
2、模拟登录163邮箱
   https://mail.163.com/




IO操作多,使用多线程

IO操作：本地磁盘IO、网络IO(requests.get())






