王伟超
wangweichao@tedu.cn
邮件：中心-班级-代码(附件)

第四阶段课程介绍 - 13个工作日
1、爬虫(SPIDER) - 7个工作日左右
   1.1> 爬虫工程师
   1.2> 作为其他岗位的附加技能(数据分析、后端开发)
2、Hadoop(大数据) - 3个工作日左右
   数据分析岗位的附加技能
3、数据结构(DataStructure) - 3个工作日左右
   所有技术岗位的附加技能

第四阶段课程特点
1、爬虫
   综合、不稳定性较强
   类、函数、字符串、列表、元组...
   MySQL、MongoDB、Redis
   多进程、多线程
   re正则表达式
   HTML
   JavaScript
   爬虫自身的巨量知识点
   图像OCR
2、Hadoop
   抽象
3、数据结构
   抽象


1、政府、公安机关的网站不要爬
2、涉及到个人隐私的数据(用户信息、简历)


1、爬虫分类
   1.1> 通用网络爬虫(搜索引擎)
   1.2> 聚焦网络爬虫(自己写的爬虫程序)
2、robots协议
   搜索引擎和网站之间签署的爬虫协议
   网站会通过robots协议来告诉搜索引擎,哪些页面可以抓,哪些页面不能抓,通用网络爬虫必须得遵守robots协议（君子协议）
3、模块使用
   resp = requests.get(url='', headers={'User-Agent':''})
   html = requests.get(url='', headers={'User-Agent':''}).text
   html = requests.get(url='', headers={'User-Agent':''}).content.decode('utf-8')
4、响应对象属性
   resp.text （字符串）
   resp.content （字节串）
   resp.status_code （HTTP响应码）
   resp.url （返回实际数据的URL地址）
5、解析模块（re）
   r_list = re.findall(regex, html, re.S)
   正则错误:  []
   正则1个分组: ['', '', '', '', ...]
   正则n个分组: [(), (), ...]
6、爬虫程序流程
   6.1> 确认数据来源：右键-查看网页源代码-搜索关键字
   6.2> 响应内容中存在：观察URL地址的规律,拼接多页的URL地址
   6.3> 正则表达式的编写
   6.4> 正常编写代码
7、数据持久化的方式
   7.1> MySQL(pymysql)
        __init__(self): 连接数据库
	save_html(self): 将数据处理成列表|元组,execute()方法
	crawl(self): 关闭游标,断开数据库的连接
   7.2> MongoDB
        __init__(self): 连接数据库(3个对象)
	save_html(self): 将数据处理成字典,存入数据库
	常用命令：mongo、show dbs、use 库名、show collections、db.集合名.count()、db.集合名.find().pretty()、db.dropDatabase()
   7.3> csv
        __init__(self): 打开文件并初始化写入对象
	save_html(self): 将数据处理成列表|元组,存入csv文件
	crawl(self): 关闭文件
8、多级页面数据抓取
   8.1> 定义功能函数
        请求功能函数、解析功能函数、md5加密功能函数... ...
   8.2> 以一级页面解析函数为主线函数
9、增量爬虫实现
   9.1> 利用redis中的集合
        redis基于内存,存取速度相当之快
	redis中自带集合数据类型,成员不能重复,适合存储请求指纹
   9.2> 原理：利用sadd()的返回值
        返回值1: 为新更新数据,需要抓取
	返回值0: 之前已抓过,结束程序!
10、目前遇到的反爬及解决方案
   10.1> 反爬: 基于headers反爬虫(User-Agent、Cookie、Referer)
         解决: 发送请求时,携带headers参数(F12抓包)
   10.2> 反爬: 基于User-Agent反爬虫 - 频率
         解决: 创建User-Agent池,每次访问随机选择UA
	       利用fake_useragent模块
	       from fake_useragent import UserAgent
	       agent = UserAgent().random
   10.3> 反爬: 对响应内容做调整(响应内容和前端页面HTML结构不一致)
         解决: 一切以响应内容为准,调整正则或xpath表达式
   10.4> 反爬: 基于IP地址频率反爬
         解决: 建立代理IP池,每次访问随机选择一个代理IP
	       (免费代理IP池、收费代理IP池)
   10.5> 反爬: JS加密 - 有道翻译
         解决: 抓取加密的JS文件,用Python实现一遍(salt、sign)
   10.6> 反爬: JS逆向(JS加密代码异常复杂) - 百度翻译
         解决: 找到对应加密的JS代码,利用pyexecjs模块进行逆向处理
   10.7> 反爬: URL地址跳转 - 民政部
         解决: 提取跳转后的URL地址,再发请求提具体数据
   10.8> 反爬: Ajax动态加载
         解决: F12抓包,确定返回实际数据的网络数据包进行分析
   10.9> 反爬: 验证码反爬
         解决: 图形验证码,使用tesseract或者在线打码
	       滑块缺口验证码,使用selenium模拟人的行为滑动
11、解析模块汇总
   11.1> 正则re ：r_list = re.findall(regex, html, re.S)
   11.2> lxml + xpath
         铭记: 只要调用了xpath,则结果一定是列表！！！！！
	 结果情况1: [] 空列表
	 结果情况2: [<element div at xxx>,<element div at xx>]
	            xpath表达式末尾没有 /text()  /@属性名
		    //div、//ul/li、//dl[@id="xxx"]/dd/div[1]/a[2]
	 结果情况3: ['', '', '', ....]
		    xpath表达式末尾有 /text()   /@属性名
		    //div/text()  //dl/dd[2]/a[1]/@href
   11.3> lxml + xpath 最常用
         eobj = etree.HTML(html)
	 div_list = eobj.xpath('//ul/li')
	 for div in div_list:
	     item = {}
	     item['name'] = div.xpath('.//p/text()')[0].strip()
12、requests模块方法及参数总结
   12.1> requests.get()
         url = ''
	 proxies = {}
	 headers = {}
	 timeout = 3
   12.2> requests.post()
         url = ''
	 data = {} #form表单数据
	 proxies = {}
	 headers = {} 
	 timeout = 3
13、数据抓取最终梳理
  确认数据来源 - 右键查看网页源代码搜索关键字
  13.1> 响应内容中存在的情况
        观察URL地址规律,想办法拼接多页的URL地址
	写正则表达式 或者 xpath表达式
	完善代码
  13.2> 响应内容中不存在的情况
	F12抓包工具进行抓包,在页面中执行某些行为
	努力寻找返回实际数据的网络数据包(All、XHR)
	分析具体网络数据包(Headers)
	如果有加密数据,进一步破解处理
14、pyexecjs模块使用 - 用于JS逆向执行JS代码
    import execjs
    with open('xxx.js', 'r') as f:
        jscode = f.read()
    # 1.创建js编译对象
    jsobj = execjs.compile(jscode)
    # 2.调用eval()方法执行JS代码
    jsobj.eval('')
15、selenium爬虫优缺点
   15.1> 优点
         简单,不用过多分析页面结构以及数据来源,操作真实的浏览器
   15.2> 缺点
         效率低(提取数据效率非常低、等待页面元素加载需要时间)
   15.3> 铭记
        写程序时,只要你点击了,或者滚动鼠标滑轮...,一定要休眠给页面元素的加载预留时间,如果不休眠可能会造成两种严重的后果,其一是抓取的数据不全,其二可能会抛出异常(NoSuchElementException)
16、浏览器对象方法
    driver = webdriver.Chrome()
    16.1> driver.get(url='')
    16.2> driver.quit()
    16.3> driver.close() ：关闭当前页
    16.4> driver.maximize_window() ：浏览器窗口最大化
    16.5> driver.page_source ：HTML结构源码(不是响应内容)
    16.6> driver.page_source.find('字符串') ：查找失败,返回-1
17、节点对象方法
    node = driver.find_element_by_xxx('')
    17.1> node.send_keys('关键字') ：向文本框发送文本
    17.2> node.click() ：单击
    17.3> node.get_attribute('属性名') ：获取节点属性值
    17.4> node.text ：获取当前节点及子节点和后代节点的文本内容
18、定位节点方法
    18.1> driver.find_element_by_xxx('') 
          结果: 1个节点对象
    18.2> driver.find_elements_by_xxx('')
          结果: 节点对象列表 []
    by_id、by_name、by_class_name、by_xpath、by_link_text、by_partical_link_text
19、selenium常用流程
    19.1> 使用场景一 : text属性有规律情况
          dd_list = driver.find_elements_by_xxx('')
	  for dd in dd_list:
	      print(dd.text) # 观察规律
    19.2> 使用场景二 : text属性没有明显规律情况
          dd_list = driver.find_elements_by_xxx('')
	  for dd in dd_list:
	      # dd.text没有明显规律怎么办？？？？
	      item = {}
	      item['name'] = dd.find_element_by_xxx('')
	      item['score'] = dd.find_element_by_xxx('')
20、selenium高级
   20.1> 设置无界面(利用options参数)
   20.2> 切换句柄(driver.switch_to.window(li[index]))
   20.3> 切换子页面(driver.switch_to.frame(iframe节点对象)
   20.4> 鼠标行为(1.实例化 2.指定行为 3.执行)
         from selenium.webdriver import ActionChains
	 ActionChains(driver).move_to_element(node).perform()
21、scrapy框架的五大组件及工作流程
   21.1> 引擎(Engine)	：整个框架核心
   21.2> 爬虫文件(Spider)：负责解析提取数据(xpath)
   21.3> 调度器(Scheduler)：维护请求队列(过滤重复请求)
   21.4> 下载器(Downloader)：发请求获取响应(response)
   21.5> 项目管道(Pipeline)：负责数据处理
   下载器中间件：调度器出队列 -> 下载器中间件(包装) -> 下载器
   蜘蛛中间件：下载器response -> 蜘蛛中间件 -> 爬虫文件(解析)

   描述：爬虫项目启动时,引擎找到爬虫文件获取第一批要抓取的URL地址,交给调度器入队列,调度器生成请求指纹出队列通过下载器中间件交给下载器去下载;下载完成后返回的response通过蜘蛛中间件交给爬虫文件;爬虫解析提取的数据交给项目管道文件去处理;继续跟进的URL地址则再次交给调度器入队列,如此循环
22、scrapy框架常用命令
   22.1> scrapy startproject 项目名
   22.2> scrapy genspider 文件名 允许抓取的域名
   22.3> scrapy crawl 爬虫名
23、scrapy项目流程
   23.1> 创建爬虫项目和爬虫文件
         scrapy startproject Tencent
	 cd Tencent
	 scrapy genspider tencent www.tencent.com
   23.2> items.py - 定义抓取的数据结构
         class TencentItem(scrapy.Item):
	     title = scrapy.Field()
	     salary = scrapy.Field()
   23.3> tencent.py - 爬虫文件解析提取数据
         import scrapy
	 from ..items import TencentItem
	 class TencentSpider(scrapy.Spider):
	      name = 'tencent'
	      allowed_domains = ['www.tencent.com']
	      start_urs = ['http://www.tencent.com/']

	      def parse(self, response):
	          """一级页面"""
		  item = TencentItem()
		  item['href'] = response.xpath('').get()

		  yield scrapy.Request(url=item['href'],
		                       meta={'item':item},
				       callback=self.detail)

	      def detail(self, response):
	          """二级页面解析函数"""
		  item = response.meta['item']
		  item['duty'] = response.xpath('').get()
		  item['require'] = response.xpath('').get()

		  yield item
   23.4> pipelines.py - 管道文件负责数据处理
         class TencentPipeline(object):
	      def process_item(self, item, spider):
	            return item
   23.5> settings.py - 全局配置
         ROBOTSTXT_OBEY = False
	 CONCURRENT_REQUESTS = 16
         DOWNLOAD_DELAY = 1
	 COOKIES_ENABLED = False
	 DEFAULT_REQUEST_HEADERS = {'Cookie':'', 'User-Agent':''}
	 ITEM_PIPELINES = {'' : 200}
	 FEED_EXPORT_ENCODING = 'utf-8'
   23.6> run.py - 运行爬虫
         from scrapy import cmdline
	 cmdline.execute('scrapy crawl tencent'.split())
24、数据提交的两种方式
   24.1> 数据交给管道文件
         yield item
   24.2> 请求交给调度器
         yield scrapy.Request(url='', meta={}, callback=self.xxx)
25、settings.py中常用的变量
   25.1> ROBOTSTXT_OBEY = False
   25.2> DOWNLOAD_DELAY = 1  #下载延迟时间
   25.3> DEFAUTL_REQUEST_HEADERS = {}
   25.4> COOKIES_ENABLED = False # False开启Cookie
   25.5> ITEM_PIPELINES = {'项目名.pipelines.类名':300}
26、爬虫项目启动的两种方式
   26.1> 方式一：start_urls = ['']
   26.2> 方式二：重写start_requests()方法
         def start_requests(self):
	     生成所有要抓取的URL地址,一次性交给调度器入队列
27、scrapy数据持久化
   27.1> csv ：scrapy crawl 爬虫名 -o xxx.csv
   27.2> json：scrapy crawl 爬虫名 -o xxx.json
               FEED_EXPORT_ENCODING = 'utf-8'
   27.3> MySQL
   27.4> MongoDB
         def open_spider(self, spider):
	     """数据库连接"""
	     pass
	 def process_item(self, item, spider):
	     """具体处理item数据,必须return item"""
	     return item
	 def close_spider(self, spider):
	     """数据库断开"""
	     pass
28、分布式爬虫原理
   28.1> 分布式爬虫原理：多台主机共享一个爬取队列
   28.1> 分布式爬虫实现：重写scrapy的调度器(利用scrapy_redis模块)
29、分布式爬虫具体实现
   29.1> 完成普通的scrapy爬虫项目 - 非分布式
   29.2> 配置settings.py,为分布式爬虫
         重新指定调度器：SCHEDULER = ''
	 重新指定去重机制：DUPEFILTER_CLASS = ''
	 指定爬取完成后不清除请求指纹：SCHEDUER_PERSIST = ''
	 指定Redis的IP地址和端口号：REDIS_HOST = '' REDIS_PORT = 6379
   29.3> 把分布式爬虫代码原封不动地拷贝到分布式爬虫地所有服务器上,所有爬虫服务器开始运行爬虫
   29.4> 效果：多台爬虫服务器开始抓取数据,并且抓取的数据不重复,数据抓取的效率提升n倍
30、redis如何设置远程连接
   Ubuntu的Redis上做如下操作：
   30.1> sudo vi /etc/redis/redis.conf
         # bind 127.0.0.1 ::1   将此行注释掉
	 protected-mode no      将默认的yes改为no
   30.2> sudo /etc/init.d/redis-server restart
   Windows上测试连接
         redis-cli -h Ubuntu的IP地址 









队列：from queue import Queue
线程：from threading import Thread
线程锁：from threading import Lock


# 把100页的URL地址存放到队列q中
q = Queue()
for page in range(1, 101):
   url = 'http://page{}.html'.format(page)
   q.put(url)

# 线程事件函数
def parse_html():
   while True:
       url = q.get()
       请求、解析、数据处理

# 创建多个线程几乎同时执行抓取任务
t_list = []
for i in range(5):
   t = Thread(target=parse_html)
   t_list.append(t)
   t.start()
 
for t in t_list:
   t.join()



OCR - 大概念,光学字符识别
tesseract-ocr - 其中1个底层识别库,开源流行
pytesseract - Python模块,调用底层的tesseract-ocr



爬虫中你遇到验证码如何处理的？
1、图形验证码
   你回答：简单图形验证码使用tesseract去处理的,
           复杂的图形验证码使用在线打码(图鉴)
   面试官：机器训练的数据是怎么弄的？
   你回答：自己训练的(前提是你真的略知一二)
           我们公司有AI部门,数据是AI部门提供给我们的

2、滑块验证码
   2.1> click_and_hold()
   2.2> move_to_element_with_offset(node,xoffset= ,yoffset= )
   2.3> move_by_offset(xoffset= , yoffset= )
   2.4> release()








